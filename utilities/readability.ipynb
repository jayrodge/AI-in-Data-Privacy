{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import spacy, csv \n",
    "from textstat.textstat import textstatistics, easy_word_set, legacy_round \n",
    "import nltk\n",
    "import re\n",
    "import io\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.matcher import PhraseMatcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Readability metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits the text into sentences, using \n",
    "# Spacy's sentence segmentation which can \n",
    "# be found at https://spacy.io/usage/spacy-101 \n",
    "def break_sentences(text): \n",
    "    nlp = spacy.load('en')\n",
    "    doc = nlp(text) \n",
    "    sentences = [sent.string.strip() for sent in doc.sents]\n",
    "    return sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns Number of Words in the text \n",
    "def word_count(text): \n",
    "    sentences = break_sentences(text) \n",
    "    words = 0\n",
    "    for sentence in sentences: \n",
    "        words += len([token for token in sentence]) \n",
    "    return words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the number of sentences in the text \n",
    "def sentence_count(text): \n",
    "    sentences = break_sentences(text) \n",
    "    return len(sentences) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns average sentence length \n",
    "def avg_sentence_length(text): \n",
    "    words = word_count(text) \n",
    "    sentences = sentence_count(text) \n",
    "    average_sentence_length = float(words / sentences) \n",
    "    return average_sentence_length "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textstat is a python package, to calculate statistics from \n",
    "# text to determine readability, \n",
    "# complexity and grade level of a particular corpus. \n",
    "# Package can be found at https://pypi.python.org/pypi/textstat \n",
    "def syllables_count(word): \n",
    "    return textstatistics().syllable_count(str(word)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the average number of syllables per \n",
    "# word in the text \n",
    "def avg_syllables_per_word(text): \n",
    "    syllable = syllables_count(text) \n",
    "    words = word_count(text) \n",
    "    ASPW = float(syllable) / float(words) \n",
    "    return legacy_round(ASPW, 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return total Difficult Words in a text \n",
    "def difficult_words(text): \n",
    "\n",
    "    # Find all words in the text \n",
    "    words = [] \n",
    "    sentences = break_sentences(text) \n",
    "    for sentence in sentences: \n",
    "        words += [token for token in sentence] \n",
    "\n",
    "    # difficult words are those with syllables >= 2 \n",
    "    # easy_word_set is provide by Textstat as \n",
    "    # a list of common words \n",
    "    diff_words_set = set() \n",
    "    \n",
    "    for word in words: \n",
    "        syllable_count = syllables_count(word) \n",
    "        if word not in easy_word_set and syllable_count >= 2: \n",
    "            diff_words_set.add(word) \n",
    "\n",
    "    return len(diff_words_set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A word is polysyllablic if it has more than 3 syllables \n",
    "# this functions returns the number of all such words \n",
    "# present in the text \n",
    "def poly_syllable_count(text): \n",
    "    count = 0\n",
    "    words = [] \n",
    "    sentences = break_sentences(text) \n",
    "    for sentence in sentences: \n",
    "        words += [token for token in sentence] \n",
    "\n",
    "\n",
    "    for word in words: \n",
    "        syllable_count = syllables_count(word)\n",
    "        if syllable_count >= 3: \n",
    "            count += 1\n",
    "    return count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flesch_reading_ease(text): \n",
    "    \"\"\" \n",
    "        Implements Flesch Formula: \n",
    "        Reading Ease score = 206.835 - (1.015 × ASL) - (84.6 × ASW) \n",
    "        Here, \n",
    "        ASL = average sentence length (number of words \n",
    "            divided by number of sentences) \n",
    "            ASW = average word length in syllables (number of syllables \n",
    "            divided by number of words) \n",
    "    \"\"\"\n",
    "    FRE = 206.835 - float(1.015 * avg_sentence_length(text)) - float(84.6 * avg_syllables_per_word(text)) \n",
    "    return legacy_round(FRE, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gunning_fog(text): \n",
    "    per_diff_words = (difficult_words(text) / word_count(text) * 100) + 5\n",
    "    grade = 0.4 * (avg_sentence_length(text) + per_diff_words)\n",
    "    return grade "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smog_index(text): \n",
    "    \"\"\" \n",
    "        Implements SMOG Formula / Grading \n",
    "        SMOG grading = 3 + ?polysyllable count. \n",
    "        Here, polysyllable count = number of words of more \n",
    "        than two syllables in a sample of 30 sentences. \n",
    "    \"\"\"\n",
    "\n",
    "    if sentence_count(text) >= 3: \n",
    "        poly_syllab = poly_syllable_count(text) \n",
    "        SMOG = (1.043 * (30*(poly_syllab / sentence_count(text)))**0.5) + 3.1291\n",
    "        return legacy_round(SMOG, 1) \n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dale_chall_readability_score(text): \n",
    "    \"\"\" \n",
    "        Implements Dale Challe Formula: \n",
    "        Raw score = 0.1579*(PDW) + 0.0496*(ASL) + 3.6365 \n",
    "        Here, PDW = Percentage of difficult words. ASL = Average sentence length \n",
    "    \"\"\"\n",
    "    words = word_count(text) \n",
    "    # Number of words not termed as difficult words \n",
    "    count = words - difficult_words(text) \n",
    "    if words > 0:\n",
    "        # Percentage of words not on difficult word list \n",
    "        per = float(count) / float(words) * 100\n",
    "\n",
    "    # diff_words stores percentage of difficult words \n",
    "    diff_words = 100 - per \n",
    "\n",
    "    raw_score = (0.1579 * diff_words) + (0.0496 * avg_sentence_length(text)) \n",
    "\n",
    "    # If Percentage of Difficult Words is greater than 5 %, then; \n",
    "    # Adjusted Score = Raw Score + 3.6365, \n",
    "    # otherwise Adjusted Score = Raw Score \n",
    "\n",
    "    if diff_words > 5:\n",
    "        raw_score += 3.6365\n",
    "\n",
    "    return legacy_round(raw_score, 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-d8b97366ca38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdale_chall_readability_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "dale_chall_readability_score(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.4"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flesch_reading_ease(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smog_index(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.10154335677305"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gunning_fog(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.74257425742574"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_sentence_length(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Stemming with NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-893715f55136>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mbFound\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cookies'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_text' is not defined"
     ]
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "#nltk.download('punkt')\n",
    "nlp = spacy.blank('en')\n",
    "\n",
    "bFound= False\n",
    "target = ps.stem('cookies')\n",
    "words = word_tokenize(new_text)\n",
    "for w in words:\n",
    "    if ps.stem(w)==target:\n",
    "     bFound = True  \n",
    "     print(bFound)\n",
    "     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Phrasematcher with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terminology_list = ['Barack Obama', 'Angela Merkel', 'Washington, D.C.']\n",
    "patterns = [nlp(text) for text in terminology_list]\n",
    "matcher.add('TerminologyList', None, *patterns)\n",
    "\n",
    "doc = nlp(u\"German Chancellor Angela Merkel and US President Barack\"\n",
    "          u\"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: how to normalize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-e3e4ceef862b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;31m#returns a numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmin_max_scaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mx_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin_max_scaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "x = df.values #returns a numpy array\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
